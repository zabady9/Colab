{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zabady9/ICPC-competition-/blob/main/CNNs_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "786Za3s526wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ede3a67-e23d-4fdd-8287-58cff2fc149b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RPwj0iLLDzT_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/kaggle_api\"   # folder path\n"
      ],
      "metadata": {
        "id": "q3jQV1MlEAD1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/data\n",
        "%cd /content/data"
      ],
      "metadata": {
        "id": "_jhm2hj5EABP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085c9064-4dcf-4d38-f054-b6330288a2c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d kritikseth/fruit-and-vegetable-image-recognition\n"
      ],
      "metadata": {
        "id": "0fgmPC5sD_-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa657059-f7fe-49f2-ba23-8f1da7268b76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fruit-and-vegetable-image-recognition.zip to /content/data\n",
            "100% 1.98G/1.98G [00:14<00:00, 130MB/s]\n",
            "100% 1.98G/1.98G [00:15<00:00, 142MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-display \n",
        "! unzip /content/data/fruit-and-vegetable-image-recognition.zip\n"
      ],
      "metadata": {
        "id": "6h2izOCAELgR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "img = cv2.imread('/content/data/Data_blance/Train/Normal/IM-0001-0001.jpeg') #open only one photo\n",
        "\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "NOeu1R1ItgC2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "2a435412-d395-497e-b45e-f74de21b75c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b27faededf5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data/Data_blance/Train/Normal/IM-0001-0001.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#open only one photo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "imges = os.listdir('/content/data/Data_blance/Train/Normal')\n",
        "imges"
      ],
      "metadata": {
        "id": "OP0SOUcdsZfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "images_0 = glob.glob('/content/data/Data_blance/Train/Normal/*.jpeg')\n",
        "images_0\n"
      ],
      "metadata": {
        "id": "8UUj-qJpsmOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/data/train'\n",
        "test_path = '/content/data/test'\n",
        "val_path = '/content/data/validation'"
      ],
      "metadata": {
        "id": "ZRbpVg6Du8pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "image_size = 224\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory( \n",
        "    train_path,\n",
        "    shuffle = True,\n",
        "    image_size=(image_size,image_size),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_path,\n",
        "    shuffle = True,\n",
        "    image_size=(image_size,image_size),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_path,\n",
        "    shuffle = True,\n",
        "    image_size=(image_size,image_size),\n",
        "    batch_size = batch_size\n",
        ")\n"
      ],
      "metadata": {
        "id": "M67_Wb-qvK0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names=train_dataset.class_names\n",
        "class_names"
      ],
      "metadata": {
        "id": "wv5J_kYzv_ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "for image_batch , label_batch in train_dataset.take(1):\n",
        "  for i in range(9):#9=3*3\n",
        "    ax = plt.subplot(3,3,i+1)\n",
        "    plt.imshow(image_batch[i].numpy().astype('uint8'))\n",
        "    plt.title(class_names[label_batch[i]],fontsize=16)\n",
        "    plt.axis('off')\n"
      ],
      "metadata": {
        "id": "lnNrwMLkwZIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "oibHbT6A2foq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image_size=200\n",
        "model = Sequential(\n",
        "    [\n",
        "        layers.Conv2D(128,9,padding='same',activation='relu',input_shape=(image_size,image_size,3)),\n",
        "        layers.MaxPool2D(),\n",
        "        # layers.Dropout(0.50),\n",
        "     \n",
        "        layers.Conv2D(64,6,padding='same',activation='relu',),\n",
        "        layers.MaxPool2D(),\n",
        "        # layers.Dropout(0.50),\n",
        "\n",
        "\n",
        "        layers.Conv2D(32,3,padding='same',activation='relu',),\n",
        "        layers.MaxPool2D(),\n",
        "        # layers.Dropout(0.50),\n",
        "\n",
        "\n",
        "        layers.Conv2D(16,3,padding='same',activation='relu',),\n",
        "        layers.MaxPool2D(),\n",
        "        # layers.Dropout(0.50),\n",
        "        layers.Conv2D(8,1,padding='same',activation='relu',),\n",
        "        layers.MaxPool2D(),\n",
        "        # layers.Dropout(0.50),\n",
        "     \n",
        "        layers.Flatten(),\n",
        "     \n",
        "        layers.Dense(200 , activation='relu'),\n",
        "        layers.Dense(128 , activation='relu'),\n",
        "        layers.Dense(64 , activation='relu'),\n",
        "        layers.Dense(32 , activation='relu'),     \n",
        "        layers.Dense(16 , activation='relu'),\n",
        "        layers.Dense(3 , activation='softmax'),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "CrAFp-rT3Ocr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "npMzSYDL6A3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "9NTIz1km6CQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Epochs = 5\n",
        "\n",
        "his = model.fit(\n",
        "    train_dataset,\n",
        " epochs = 15,\n",
        " batch_size = batch_size,\n",
        " validation_data = val_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "qGe4tSf_6TVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "his.history"
      ],
      "metadata": {
        "id": "iJpNWfN-6Un4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = his.history['accuracy']\n",
        "val_acc = his.history['val_accuracy']\n",
        "loss = his.history['loss']\n",
        "val_loss = his.history['val_loss']"
      ],
      "metadata": {
        "id": "C6hUC4rR9uwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "id": "uoWAeB0b_8Pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model,img):\n",
        "  img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
        "  img_array = tf.expand_dims(img_array,0)\n",
        "\n",
        "  predictions = model.predict(img_array)\n",
        "  predictions_class = class_names[np.argmax(predictions[0])]\n",
        "  conf = round(100*(np.max(predictions[0])),2)\n",
        "  return predictions_class , conf"
      ],
      "metadata": {
        "id": "qWJBen01__eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "for images , labels in test_dataset.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3,3,i+1)\n",
        "    plt.imshow(images[i].numpy().astype('uint8'))\n",
        "    pred,conf = predict(model,images[i].numpy())\n",
        "    acu_cl = class_names[labels[i]]\n",
        "\n",
        "    plt.title(f\"Acual : {acu_cl},\\n Pred : {pred}\\n conf : {conf}\",fontsize=16)\n",
        "    plt.axis('off')\n"
      ],
      "metadata": {
        "id": "Xb0JacVqLCOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XaO1JO20LDso"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}